{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 1 in Assessment 1\n",
    "#### Student Name: Swaraj Purohit\n",
    "#### Student ID: 29286174\n",
    "\n",
    "\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "\n",
    "* re (for regular expression, included in Anaconda Python 3) \n",
    "* json (for writing to a json file, included in Anaconda Python 3) \n",
    "\n",
    "note: The cells need to be run in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "We are loading the data and splitting it into individual job postings. We can then view some postings to get an idea of how the data is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('29286174.dat', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = file.read().split('------------------------------')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del jobs[32445]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Regexs\n",
    "\n",
    "Right now the aim is to create a regular expression to identify each of the subtitles (ID, about, title, location,etc.). We start of by creating a very broad expression (eg: for job description we start with '\\n.*[Dd][Ee][Ss].**:') which surely captures the subtitle but may also capture several other words as well. However, this gives a way to view all the tags so as to converge on a better regex. We try to converge on the optimal regex by running the broad regex in batches of 100 and then running the new regex on the batch to make sure that the required words are not eliminated. This method is used to converge to optimal regexs to isolate the required subheadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_id = r'ID:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_about = r'\\n(?:(?:[Aa][Bb][Oo][Uu][Tt].{0,9})|(?:[Cc]*[Oo]*[Mm]*[Pp]*[Aa]*[Nn]*[Yy]*[Ss]*_*[Ii][Nn][Ff][Oo])|(?:[Cc][Oo][Mm][Pp][Aa][Nn][Yy][Ss]*))\\s?:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_title = r'\\n.*(?:(?:[Tt][Ii][Tt][Ll][Ee][Ss]*)|(?:_[Tt])|(?:[Tt][Tt][Ll]))\\s*?:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_loc = r'\\n.*(?:(?:[Ll][Oo][Cc][Aa][Tt][Ii][Oo][Nn][Ss]*)|(?:_[Ll][Oo][Cc][Ss]*))\\s*?:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_date = r'\\n.*(?:(?:[Dd][Aa][Tt]*[Ee]*[sS]*\\s*?:)|(?:[Ss][Tt][Aa][Rr][Tt][S]*\\s*?:))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_desc = r'\\n(?:[Jj][Oo][Bb])?[_\\s]?[Dd][Ee][Ss][Cc]*[Rr]*[Ii]*[Pp]*[Tt]*[Ii]*[oO]*[Nn]*[Ss]*\\s*?:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_resp = r'\\n(?:(?:[Jj][Oo][Bb])?[_\\s]*[Rr][Ee][Ss][Pp][Ss]?(?:[Oo][Nn][Ss][Ii][Bb][Ii][Ll][Ii][Tt].{0,3})?:)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_qual = r'\\n(?:(?:[Rr][Ee][Qq][_\\s]?[uU]?[Ii]?[Rr]?[Ee]?[Dd]?[_\\s]?[Qq][Uu][Aa][Ll]\\w{0,10}?:)|(?:[Qq][Uu][Aa][Ll]\\w{0,11}?:))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dl = r'\\n(?:.{0,12}[Dd][Ee]*[Aa]*[Dd]*_*[Ll]).{0,5}?:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sal = r'\\n.{0,4}(?:(?:[Rr][Ee][Mm][Uu][nN][Ee]*)|(?:[Ss][aA][Ll][Aa][Rr][Yy])|(?:_[Ss][aA][Ll])).{0,8}?:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_proc = r'\\n(?:(?:[Jj][Oo][Bb][_\\s])?[Pp][Rr][Oo][Cc]?[Ee]?[Dd]?[Uu]?[Rr]?[Ee]?[Ss]?):'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making regex to detect any subheading (tag)\n",
    "\n",
    "r_tag = r_id + '|' + r_about +'|'+ r_title +'|'+ r_loc +'|'+ r_date +'|'+ r_desc +'|'+ r_resp +'|'+ r_qual +'|'+ r_dl +'|'+ r_sal +'|'+ r_proc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n_description:']\n",
      "['\\nJOB_DESC:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\njob_desc:']\n",
      "['\\n_description:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\n_description:']\n",
      "['\\njob_desc:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "[]\n",
      "['\\nJOB_DESC:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nJOB_DESC:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB_DESC:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\njob_desc:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\njob_desc:']\n",
      "[]\n",
      "[]\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\njob_desc:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB_DESC:']\n",
      "['\\nJOB_DESC:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\njob_desc:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\njob_desc:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "[]\n",
      "['\\n_description:']\n",
      "[]\n",
      "['\\nJOB_DESC:']\n",
      "['\\nJOB_DESC:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB_DESC:']\n",
      "['\\njob_desc:']\n",
      "[]\n",
      "['\\njob_desc:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB_DESC:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\njob_desc:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB_DESC:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:', '\\nJOB DESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nDESCRIPTION:', '\\nJOB DESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\n_description:']\n",
      "['\\nJOB DESCRIPTION:']\n",
      "[]\n",
      "['\\nJOB_DESC:']\n",
      "['\\n_description:']\n",
      "['\\nDESCRIPTION:']\n",
      "['\\nJOB DESCRIPTION:']\n"
     ]
    }
   ],
   "source": [
    "for each in jobs[11400:11500]:\n",
    "    print(re.findall(r_desc, each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Checking to see how many matches a regex returns.\n",
    "\n",
    "Ideally we would want a regex to return zero or one match. Checking to make sure this is true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ensure no more than 2 matches. Run all regexes in this code to ensure no regex returns more than 2 matches.\n",
    "for each in jobs:\n",
    "    if len(re.findall(r_proc, each)) > 2:\n",
    "        print(jobs.index(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "81\n",
      "82\n",
      "96\n",
      "210\n",
      "734\n",
      "768\n",
      "861\n",
      "1376\n",
      "1395\n",
      "1488\n",
      "1662\n",
      "1759\n",
      "1796\n",
      "1893\n",
      "1962\n",
      "1966\n",
      "1976\n",
      "2066\n",
      "2257\n",
      "2545\n",
      "2591\n",
      "2614\n",
      "2710\n",
      "2916\n",
      "2952\n",
      "3183\n",
      "3229\n",
      "3248\n",
      "3637\n",
      "3916\n",
      "3929\n",
      "3955\n",
      "4207\n",
      "4227\n",
      "4238\n",
      "4247\n",
      "4585\n",
      "4666\n",
      "4702\n",
      "4708\n",
      "4731\n",
      "4837\n",
      "4884\n",
      "4899\n",
      "5144\n",
      "5247\n",
      "5366\n",
      "5453\n",
      "5717\n",
      "5726\n",
      "5785\n",
      "5842\n",
      "5916\n",
      "6038\n",
      "6147\n",
      "6252\n",
      "6269\n",
      "6287\n",
      "6356\n",
      "6416\n",
      "6538\n",
      "6566\n",
      "6601\n",
      "6650\n",
      "7000\n",
      "7066\n",
      "7199\n",
      "7598\n",
      "7858\n",
      "7903\n",
      "7962\n",
      "8184\n",
      "8301\n",
      "8415\n",
      "8449\n",
      "8977\n",
      "9122\n",
      "9368\n",
      "9379\n",
      "9515\n",
      "9585\n",
      "9794\n",
      "9853\n",
      "9916\n",
      "9939\n",
      "10069\n",
      "10093\n",
      "10332\n",
      "10351\n",
      "10450\n",
      "10532\n",
      "10551\n",
      "10575\n",
      "10579\n",
      "10615\n",
      "10699\n",
      "10727\n",
      "10753\n",
      "10797\n",
      "10853\n",
      "10977\n",
      "11004\n",
      "11049\n",
      "11076\n",
      "11145\n",
      "11238\n",
      "11352\n",
      "11482\n",
      "11490\n",
      "11718\n",
      "11821\n",
      "11887\n",
      "12055\n",
      "12148\n",
      "12303\n",
      "12369\n",
      "12414\n",
      "12464\n",
      "12486\n",
      "12639\n",
      "12666\n",
      "12790\n",
      "12897\n",
      "12900\n",
      "13025\n",
      "13030\n",
      "13127\n",
      "13254\n",
      "13271\n",
      "13507\n",
      "13525\n",
      "13542\n",
      "13714\n",
      "13795\n",
      "13973\n",
      "14080\n",
      "14191\n",
      "14366\n",
      "14381\n",
      "14435\n",
      "14668\n",
      "14733\n",
      "14751\n",
      "14787\n",
      "14840\n",
      "14966\n",
      "15225\n",
      "15353\n",
      "15358\n",
      "15612\n",
      "15978\n",
      "16261\n",
      "16361\n",
      "16408\n",
      "16417\n",
      "16458\n",
      "16503\n",
      "16532\n",
      "16568\n",
      "16683\n",
      "16741\n",
      "16911\n",
      "17099\n",
      "17100\n",
      "17344\n",
      "17636\n",
      "17781\n",
      "17816\n",
      "17881\n",
      "17895\n",
      "17947\n",
      "17978\n",
      "18188\n",
      "18195\n",
      "18238\n",
      "18597\n",
      "18810\n",
      "18822\n",
      "18958\n",
      "19033\n",
      "19230\n",
      "19437\n",
      "19526\n",
      "19759\n",
      "19941\n",
      "20245\n",
      "20258\n",
      "20259\n",
      "20294\n",
      "20362\n",
      "20550\n",
      "20795\n",
      "20877\n",
      "20984\n",
      "21065\n",
      "21115\n",
      "21199\n",
      "21308\n",
      "21349\n",
      "21468\n",
      "21487\n",
      "21517\n",
      "21530\n",
      "21595\n",
      "21768\n",
      "21829\n",
      "21886\n",
      "21924\n",
      "22289\n",
      "22434\n",
      "22693\n",
      "22718\n",
      "22749\n",
      "22767\n",
      "22879\n",
      "22925\n",
      "23164\n",
      "23268\n",
      "23346\n",
      "23412\n",
      "23504\n",
      "23634\n",
      "23707\n",
      "23720\n",
      "23999\n",
      "24178\n",
      "24293\n",
      "24384\n",
      "24501\n",
      "24551\n",
      "24708\n",
      "24722\n",
      "24775\n",
      "24817\n",
      "24871\n",
      "25331\n",
      "25686\n",
      "25971\n",
      "26083\n",
      "26294\n",
      "26304\n",
      "26343\n",
      "26537\n",
      "26553\n",
      "26570\n",
      "26571\n",
      "26974\n",
      "27040\n",
      "27156\n",
      "27415\n",
      "27523\n",
      "27563\n",
      "27657\n",
      "27673\n",
      "27709\n",
      "27850\n",
      "27939\n",
      "28229\n",
      "28230\n",
      "28320\n",
      "28428\n",
      "28491\n",
      "28557\n",
      "28565\n",
      "28588\n",
      "28702\n",
      "28758\n",
      "29072\n",
      "29177\n",
      "29182\n",
      "29313\n",
      "29413\n",
      "29422\n",
      "29456\n",
      "29506\n",
      "29551\n",
      "29644\n",
      "29699\n",
      "29713\n",
      "30057\n",
      "30246\n",
      "30421\n",
      "30587\n",
      "30695\n",
      "30709\n",
      "30854\n",
      "30864\n",
      "31023\n",
      "31038\n",
      "31264\n",
      "31344\n",
      "31450\n",
      "31579\n",
      "31659\n",
      "31662\n",
      "31805\n",
      "31815\n",
      "31928\n",
      "31989\n",
      "32055\n",
      "32154\n",
      "32212\n",
      "32331\n",
      "32349\n",
      "32359\n"
     ]
    }
   ],
   "source": [
    "#Checking to see if any regex returns 2 matches.\n",
    "for each in jobs:\n",
    "    if len(re.findall(r_desc, each)) == 2:\n",
    "        print(jobs.index(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding why there are 2 matches for a regex.\n",
    "r_loc, r_date, r_desc, r_resp, r_qual, r_dl, r_proc return 2 matches. Since, the number of jobs which give 2 matches is less we can manually see the pattern for all except r_desc.\n",
    "\n",
    "For r_loc, r_date, r_resp and r_qual we see that the second match is not correct and needs to be deleted.\n",
    "For r_proc we need to delete the first match.\n",
    "\n",
    "We observe that r_desc returns 2 matches for several job postings. Observing some shows that this is because many postings have a job description at the end. The following code gives no output. This confirms that this is true for all. Although we observe that the second match of job description does not seem to be related to the job posting we choose to include it as we are not concerned with the meaning of the text we extract for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "for each in jobs:\n",
    "    if len(re.findall(r_desc, each)) == 2:\n",
    "        A.append(jobs.index(each))\n",
    "for each in A:\n",
    "    if re.findall(r_tag, jobs[each])[len(re.findall(r_tag, jobs[each]))-1] != '\\nJOB DESCRIPTION:':\n",
    "        print(each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID:',\n",
       " '\\nABOUT:',\n",
       " '\\nJOB_T:',\n",
       " '\\nSTART DATE:',\n",
       " '\\nLOCATION:',\n",
       " '\\nDEAD_LINE:',\n",
       " '\\nREQUIRED QUALIFICATIONS:',\n",
       " '\\nJOB_RESPS:',\n",
       " '\\nprocedures:',\n",
       " '\\njob_desc:',\n",
       " '\\nSALARY:',\n",
       " '\\nJOB_PROC:']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r_tag,jobs[23230])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating function to deal with observations made in the previous section.\n",
    "\n",
    "This function cleans up the tags obtained by using re.findall(r_tag, job). Cleaning means the duplicate captures are removed using the pattern noticed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a function which removes unwanted tags based on the observations made previously.\n",
    "regexs = [r_id, r_about, r_title, r_loc, r_date, r_desc, r_resp, r_qual, r_dl, r_sal, r_proc]\n",
    "delsecond = [r_loc, r_date, r_resp, r_qual]\n",
    "def tag_cleaner(job, tags):\n",
    "\n",
    "    for regex in regexs:\n",
    "\n",
    "        if len(re.findall(regex, job)) == 2 and regex in delsecond:\n",
    "            rem = re.findall(regex,job)[1]\n",
    "            temp = tags[::-1]\n",
    "            temp.remove(rem)\n",
    "            tags = temp[::-1]\n",
    "        if len(re.findall(regex, job)) == 2 and regex == r_proc:\n",
    "            rem = re.findall(regex,job)[0]\n",
    "            tags.remove(rem)\n",
    "        if len(re.findall(regex, job)) == 2 and regex == r_desc:\n",
    "            pass\n",
    "    return tags  \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to extract required text for a particular subheading regex. Find all tags, find the tag which \n",
    "# matches required regex, find the tag after that and extract everything in between the two.\n",
    "# Special case when two descriptions are there is incorporated in the function.\n",
    "def extractor(regex, lst):\n",
    "    out = []\n",
    "    for each in lst:\n",
    "        if len(re.findall(regex, each)) == 0: #Case when no match\n",
    "            out.append('N/A')\n",
    "        if len(re.findall(regex, each)) > 0: #Case when 1 or 2 matches\n",
    "            start = re.findall(regex, each)[0]\n",
    "            tags = re.findall(r_tag, each)\n",
    "            tags = tag_cleaner(each, tags)\n",
    "            for i in range(len(tags)):\n",
    "                if tags[i] == start and i != len(tags)-1:\n",
    "                    end = tags[i+1]\n",
    "                    break\n",
    "                if tags[i] == start and i == len(tags)-1:\n",
    "                    end = '$'\n",
    "            extractor = start + '(.*?)' + end\n",
    "            out.append(re.findall(extractor , each, flags =16)[0])\n",
    "        \n",
    "        if len(re.findall(regex, each)) > 1 and regex  == r_desc: #Case to occupy 2 descriptions\n",
    "            start = re.findall(regex, each)[0]\n",
    "            tags = re.findall(r_tag, each)\n",
    "            tags = tag_cleaner(each, tags)\n",
    "            for i in range(len(tags)):\n",
    "                if tags[i] == start and i != len(tags)-1:\n",
    "                    end = tags[i+1]\n",
    "                    break\n",
    "                if tags[i] == start and i == len(tags)-1:\n",
    "                    end = '$'\n",
    "            extractor1 = start + '(.*?)' + end\n",
    "            if re.findall(regex, each)[0] == re.findall(regex, each)[1]:#If both desc matches are same    \n",
    "                extractor2 = tags[-1] + '(.*?)' + '$'\n",
    "                temp = re.findall(extractor2 , each, flags =16)[0]\n",
    "                del out[-1]\n",
    "                out.append(re.findall(extractor1 , each, flags =16)[0] +'\\n-'+ re.findall(extractor2 , temp, flags =16)[0])\n",
    "            if re.findall(regex, each)[0] != re.findall(regex, each)[1]:#If desc matches are not same\n",
    "                extractor2 = tags[-1] + '(.*?)' + '$'\n",
    "                del out[-1]\n",
    "                out.append(re.findall(extractor1 , each, flags =16)[0] +'\\n-'+ re.findall(extractor2 , each, flags =16)[0])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting all the texts in respective lists. This may take 10-12 mins.\n",
    "all_ID = extractor(r_id, jobs)\n",
    "all_about = extractor(r_about, jobs)\n",
    "all_title = extractor(r_title, jobs)\n",
    "all_loc = extractor(r_loc, jobs)\n",
    "all_date = extractor(r_date, jobs)\n",
    "all_desc = extractor(r_desc, jobs)\n",
    "all_resp = extractor(r_resp, jobs)\n",
    "all_qual = extractor(r_qual, jobs)\n",
    "all_dl = extractor(r_dl, jobs)\n",
    "all_sal = extractor(r_sal, jobs)\n",
    "all_proc = extractor(r_proc, jobs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to the required file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the XML and a dictionary which will be dumped in the json format.\n",
    "xml_file = open('29286174.xml', 'w')\n",
    "xml_file.write('<listings>\\n')\n",
    "jobs_dict = {}\n",
    "for i in range(len(all_ID)):\n",
    "    job_dict = {}\n",
    "    job_dict['id'] = all_ID[i]\n",
    "    job_dict['title'] = all_title[i]\n",
    "    job_dict['location'] = all_loc[i]\n",
    "    job_dict['job_descriptions'] = {'description': all_desc[i].split('\\n-') }\n",
    "    job_dict['job_responsibilities'] = {'responsibility': all_resp[i].split('\\n-') }\n",
    "    job_dict['required_qualifications'] = {'qualification': all_qual[i].split('\\n-') }\n",
    "    job_dict['salary'] = all_sal[i]\n",
    "    job_dict['application_procedure'] = all_proc[i]\n",
    "    job_dict['start_date'] = all_date[i]\n",
    "    job_dict['application_deadline'] = all_dl[i]\n",
    "    job_dict['about_company'] = all_about[i]\n",
    "    jobs_dict['listing'+str(i)] = [job_dict]\n",
    "    \n",
    "    xml_file.write('\\t<listing id=\"{0}\">\\n'.format(all_ID[i]))\n",
    "    xml_file.write('\\t\\t<title>{0}</title>\\n'.format(all_title[i]))\n",
    "    xml_file.write('\\t\\t<location>{0}</location>\\n'.format(all_loc[i]))\n",
    "    \n",
    "    xml_file.write('\\t\\t<job_descriptions>\\n')\n",
    "    for each in all_desc[i].split('\\n-'):\n",
    "        xml_file.write('\\t\\t\\t<description> {0} </description>\\n'.format(each))\n",
    "    xml_file.write('\\t\\t</job_descriptions>\\n')\n",
    "    \n",
    "    xml_file.write('\\t\\t<job_responsibilities>\\n')\n",
    "    for each in all_resp[i].split('\\n-'):\n",
    "        xml_file.write('\\t\\t\\t<responsibility> {0} </responsibility>\\n'.format(each))\n",
    "    xml_file.write('\\t\\t</job_responsibilities>\\n')\n",
    "    \n",
    "    xml_file.write('\\t\\t<required_qualifications>\\n')\n",
    "    for each in all_qual[i].split('\\n-'):\n",
    "        xml_file.write('\\t\\t\\t<qualification> {0} </qualification>\\n'.format(each))\n",
    "    xml_file.write('\\t\\t</required_qualifications>\\n')\n",
    "    \n",
    "    xml_file.write('\\t\\t<salary> {0} </salary>\\n'.format(all_sal[i]))\n",
    "    xml_file.write('\\t\\t<application_procedure> {0} </application_procedure>\\n'.format(all_proc[i]))\n",
    "    xml_file.write('\\t\\t<start_date> {0} </start_date>\\n'.format(all_date[i]))\n",
    "    xml_file.write('\\t\\t<application_deadline> {0} </application_deadline>\\n'.format(all_dl[i]))\n",
    "    xml_file.write('\\t\\t<about_company> {0} </about_company>\\n'.format(all_about[i]))\n",
    "    xml_file.write('\\t</listing>\\n')\n",
    "    \n",
    "    \n",
    "json_dict = {'listings':jobs_dict}\n",
    "xml_file.write('</listings>')\n",
    "xml_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_file = open('job postings.json', 'w')\n",
    "json.dump(json_dict, json_file, indent = 4)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Editing the json file. Need to convert listing0, linsting1, listing3.... to listing\n",
    "file1  = open('job postings.json', 'r')\n",
    "json_txt = file1.read()\n",
    "file.close()\n",
    "r_lis = r'\"(listing\\d+)\"'\n",
    "file2 = open('29286174.json', 'w') \n",
    "file2.write(re.sub(r_lis, '\"listing\"', json_txt))\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
